---
author: zkslr
pubDatetime: 2026-1-18T10:03:00.000+08:00
title: 概率论与统计学学习笔记（持续更新）
featured: false
draft: false
tags:
  - ai
description: 学习笔记
---

# 概率论基础
## 随机试验
随机试验是指一个在相同条件下进行，但每次试验结果不确定的实验。它具有以下特征：
* 每次试验的结果是不可预测的。
* 试验在重复进行时，可以得到不同的结果。

例子： 掷骰子，抛硬币
## 样本空间
样本空间是指随机试验所有可能的结果的集合。对于一个随机试验，样本空间包含了所有可能发生的结果。
* 掷骰子：样本空间是 $Ω={1,2,3,4,5,6}$
* 抛硬币：样本空间是 $Ω={H,T}$

## 事件
事件是样本空间中的一个子集。事件可以是一个单一的结果，也可以是多个结果的集合。
* 掷骰子：事件A可以定义为“掷出偶数点数”，则 $A={2,4,6}$
* 抛硬币：事件B可以定义为“抛出正面”，则 $B={H}$
事件的运算：并集、交集、补集

# 概率的定义和基本性质
## 概率的定义
概率是指某一事件发生的可能性，它是一个介于0和1之间的数。常见的定义有： 

**经典概率：** 当所有事件的可能性相等时，事件发生的概率等于该事件的 favorable outcomes（有利结果）与样本空间中所有可能结果的比值。 

**公式：**   $P(A) = {{事件A发生的 favorable outcomes} \over {样本空间中所有可能的outcomes}}$
## 概率的基本性质
1. 非负性：对于任何事件 A，其概率总是大于或等于0： $P(Ω)=1$
2. 规范化：样本空间中的所有事件的概率之和为1: $P(Ω)=1$ 
3. 加法法则： 对于两个事件A和B，事件  $(A \cup B)$的概率为： $P(A \cup B) =P(A)+P(B)-P(A \cap B)$。这里需要减去 $P(A \cap B)$,因为要考虑A和B都发生的情况
4. 乘法法则： 对于两个事件A和B，事件  $A \cap B$的概率为： $P(A \cap B) =P(A) \cdot P(B)$
5. 独立性： 事件A和B如果满足 $P(A \cap B) =P(A) \cdot P(B)$，则称 A和B是独立事件。

# 条件概率、全概率公式、贝叶斯定理
## 条件概率（Conditional Probability）
事件B已经发生的前提下，事件A发生的概率： $P(A \mid B) =P(A \cap B) \over P(B) P(B)>0$
直觉： 将样本空间缩小到B，再看A在B这个新空间占多少
乘法公式：  移项得到：  $P(A \cap B) = P(A \mid B)P(B)$
同理： $P(A \cap B) = P(B \mid A)P(A)$
特殊情况： 若A,B独立，$P(A \mid B) =P(A)$

## 全概率公式
当事件 $B_1,B_2,...,B_n互斥且完备（分割样本空间）
* 互斥： $B_i \cap B_j = \emptyset$
* 完备： $\cup_iB_i= \Omega$
则对于任意事件A： $P(A) = \sum_{i=1}^{n}P(A \mid B_i)P(B_i)$

## 贝叶斯公式 !!!!重点
核心：把“因→果”的概率 $P(A \mid B)$转化成“果→因”的概率$P(B \mid A)$

由乘法公式  $P(A \cap B) = P(A \mid B)$ $P(B) = P(B \mid A)P(A)$ 

得到 $P(B \mid A) = {P(A \mid B)P(B)} \over P(A)$ 
若B有多类来源( $B_i$分割样本空间)，则结合全概率公式： 
$$P(B_k \mid A) = {P(A \mid B_k)P(B_k)} \over \sum_{i=1}^{n}P(A \mid B_i)P(B_i)$$ 

经典例子： 一个检查结果是阳性，真实得病的概率。
前提： 患病率（先验）： $P(D)=0.01$,灵敏度（得病了，然后去检查是阳性概率是0.99） $P(+ \mid D)=0.99$,假阳性率（没得病，去检查是阳性概率0.05）：P(+ \mid \overline{D})=0.05. 

计算： 先用全概率算阳性概率： 
$$P(+)=P(+ \mid D)P(D)+P(+ \mid \overline{D})P(\overline{D}) = 0.99 \cdot 0.01 + 0.05 \cdot 0.99 = 0.0594$$ 

然后计算阳性之后真有病的概率： $P(D \mid +) = {{P(+ \mid D)P(D)} \over {P(+)}} = {{0.99 \cdot 0.01} \over 0.0594} \approx 0.1667$

### 贝叶斯思考
贝叶斯是可重复的更新规则，考虑状态（Hypothesis）和证据（Data） ，通过新信息的更新去影响对某个事件的信念
状态就是 “患病”，“市场将上涨”，“硬币偏向正面” ,,P(H)其实是你对H的相信程度
证据就是“检测结果是阳性”，“当前价格在涨”，“抛了10次出现8次正面”   P(D)是证据的总概率
要做的事情： 根据证据D去反推原因H的可能性。就是说 检测结果阳了，原因是患病或者设备坏了或者其它？ ，计算的是 $P(H \mid D)$
我们需要用似然 $P(D \mid H)$,就是在先验 $P(H)$ 的前提下，证据出现的概率去除以所有证据在所有原因下的总概率

# 随机变量与分布
## 随机变量
随机变量（Random Variable）是一个将随机试验的结果映射为一个数值的函数。它的结果是不确定的，并且每次试验可能会有不同的输出。随机变量可以分为两种类型：
1. **离散型随机变量：** 它的取值是有限个或可列举的。例如，掷骰子时，随机变量可以取1到6中的任意一个整数。
2. **连续型随机变量：** 它的取值是无限多的，并且可以在某个区间内取值。例如，温度、时间、长度等。
### 离散型随机变量



